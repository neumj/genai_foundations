{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQLFotRZPMaG"
   },
   "source": [
    "Let’s implement a basic attention mechanism in Python to reinforce the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QonBRCaJPMaH",
    "outputId": "a95af3cd-638c-4ff1-b026-b814d5d427ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores: [ 1 -1  2]\n",
      "Attention Weights: [0.25949646 0.03511903 0.70538451]\n",
      "Attention Output: [-0.81867124]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define query, keys, and values as vectors\n",
    "query = np.array([1, 0, -1])  # Represents \"love\"\n",
    "keys = np.array([[1, 1, 0],  # Represents \"pizza\"\n",
    "                 [0, -1, 1],  # Represents \"but\"\n",
    "                 [1, 0, -1]])  # Represents \"olives\"\n",
    "values = np.array([[5], [0], [-3]])  # Sentiment scores for \"pizza,\" \"but,\" \"olives\"\n",
    "\n",
    "# Calculate attention scores (dot product of query and keys)\n",
    "scores = np.dot(keys, query)\n",
    "\n",
    "# Apply softmax to get weights\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "weights = softmax(scores)\n",
    "\n",
    "# Weighted sum of values\n",
    "attention_output = np.dot(weights, values)\n",
    "\n",
    "print(\"Attention Scores:\", scores)\n",
    "print(\"Attention Weights:\", weights)\n",
    "print(\"Attention Output:\", attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy4GBnqgPMaJ"
   },
   "source": [
    "To improve stability for high-dimensional vectors, we scale the dot product by dividing by the square root of the key dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aokQ07hPMaJ",
    "outputId": "876a0a7f-b6e6-4b37-8b21-cbb159f83905"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define query, keys, values, and scaling factor\n",
    "query = np.array([1, 0, 1])\n",
    "keys = np.array([[1, 0, 1],\n",
    "                 [0, 1, 0],\n",
    "                 [1, 0, -1]])\n",
    "values = np.array([[5], [0], [-3]])\n",
    "scale = np.sqrt(query.shape[0])  # d_k = dimensionality of the query\n",
    "\n",
    "# Calculate scaled attention scores\n",
    "scores = np.dot(keys, query) / scale\n",
    "\n",
    "# Apply softmax to get weights\n",
    "weights = softmax(scores)\n",
    "\n",
    "# Weighted sum of values\n",
    "attention_output = np.dot(weights, values)\n",
    "\n",
    "print(\"Scaled Attention Scores:\", scores)\n",
    "print(\"Attention Weights:\", weights)\n",
    "print(\"Attention Output:\", attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b04tlJcPMaJ"
   },
   "source": [
    "## Self Attention\n",
    "Let’s implement a simple self-attention mechanism for a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHtUf4V3PMaJ",
    "outputId": "c57a7333-a2de-48c2-dbb6-a711c68e6df9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define structured sentence embeddings (each row represents a word)\n",
    "# The embeddings represent semantic roles:\n",
    "# \"The\" (determiner), \"cat\" (subject noun), \"sat\" (verb), \"on\" (preposition), \"mat\" (object noun)\n",
    "sentence_embeddings = np.array([\n",
    "    [0.1, 0.1, 0.2],  # \"The\"  (low influence determiner)\n",
    "    [0.9, 0.8, 0.7],  # \"cat\"  (subject noun, strong influence)\n",
    "    [0.8, 0.9, 0.8],  # \"sat\"  (verb, central word, strong influence)\n",
    "    [0.2, 0.2, 0.3],  # \"on\"   (preposition, weak influence but linked to \"mat\")\n",
    "    [0.7, 0.6, 0.9]   # \"mat\"  (object noun, linked to \"on\" and \"sat\", but not \"the\")\n",
    "])\n",
    "\n",
    "# Define structured weight matrices for Query (Q), Key (K), and Value (V)\n",
    "# These weights are manually structured to enhance word relationships\n",
    "\n",
    "# Query weight matrix (W_q) (3x3)\n",
    "# Controls how much influence each word has when \"asking for context\"\n",
    "W_q = np.array([\n",
    "    [0.6, 0.2, 0.1],  # Slight attention to structure words\n",
    "    [0.8, 0.7, 0.6],  # \"cat\" has high query influence\n",
    "    [0.7, 0.9, 0.8]   # \"sat\" is the central querying word\n",
    "])\n",
    "\n",
    "# Key weight matrix (W_k) (3x3)\n",
    "# Controls how words \"store\" information for queries to access\n",
    "W_k = np.array([\n",
    "    [0.6, 0.3, 0.2],  # \"The\" contributes weakly to keys\n",
    "    [0.8, 0.7, 0.5],  # \"cat\" stores important information\n",
    "    [0.7, 0.9, 0.6]   # \"sat\" stores strong reference points\n",
    "])\n",
    "\n",
    "# Value weight matrix (W_v) (3x3)\n",
    "# Controls how much information each word contributes to the final representation\n",
    "W_v = np.array([\n",
    "    [0.2, 0.5, 0.3],  # \"The\" contributes little meaning\n",
    "    [0.7, 0.8, 0.6],  # \"Cat\" contributes strongly\n",
    "    [0.8, 0.9, 0.7]   # \"Sat\" contributes highly\n",
    "])\n",
    "\n",
    "# Compute Queries (Q), Keys (K), and Values (V) for each word\n",
    "Q = sentence_embeddings @ W_q  # Transform embeddings into Queries\n",
    "K = sentence_embeddings @ W_k  # Transform embeddings into Keys\n",
    "V = sentence_embeddings @ W_v  # Transform embeddings into Values\n",
    "\n",
    "# Compute attention scores using scaled dot-product attention\n",
    "scores = Q @ K.T  # Compute raw attention scores (similarity between Q and K)\n",
    "\n",
    "# Scale scores to stabilize training (common practice in attention models)\n",
    "d_k = K.shape[1]  # Dimension of keys\n",
    "scores /= np.sqrt(d_k)  # Scaling factor\n",
    "\n",
    "# Apply softmax to obtain normalized attention weights\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max value for numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)  # Normalize rows\n",
    "\n",
    "weights = softmax(scores)  # Compute final attention weights\n",
    "\n",
    "# Compute weighted sum of values to get attention output\n",
    "attention_output = weights @ V  # Weighted combination of V based on attention\n",
    "\n",
    "# Print the attention matrix (how words attend to each other)\n",
    "print(\"Attention Weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Print the final contextualized word representations\n",
    "print(\"Attention Output:\")\n",
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwD5IpOnPMaK"
   },
   "source": [
    "## Multi-Head Attention\n",
    "Let’s implement a simple version of multi-head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Pg9DIMLPMaL",
    "outputId": "d2e01a79-0284-40d8-e7ca-e3395434935e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define sentence embeddings for each word in \"The cat sat on the mat\"\n",
    "# Each word is represented as a 3-dimensional vector (for simplicity)\n",
    "sentence_embeddings = np.array([\n",
    "    [0.1, 0.1, 0.2],  # \"The\"  (low influence)\n",
    "    [0.9, 0.8, 0.7],  # \"cat\"  (high influence)\n",
    "    [0.8, 0.9, 0.8],  # \"sat\"  (central word)\n",
    "    [0.2, 0.2, 0.3],  # \"on\"   (context word)\n",
    "    [0.7, 0.6, 0.9]   # \"mat\"  (linked to \"sat\" and \"on\")\n",
    "])\n",
    "\n",
    "# Multi-head attention will have two heads for this example\n",
    "num_heads = 2\n",
    "head_dim = sentence_embeddings.shape[1] // num_heads  # Dimension per head\n",
    "\n",
    "# Function to create weight matrices for each head\n",
    "# We'll create separate W_q, W_k, W_v for each head\n",
    "np.random.seed(42)  # Seed for reproducibility\n",
    "def generate_weights(num_heads, head_dim):\n",
    "    return [\n",
    "        (np.random.rand(head_dim, head_dim),  # W_q for this head\n",
    "         np.random.rand(head_dim, head_dim),  # W_k for this head\n",
    "         np.random.rand(head_dim, head_dim))  # W_v for this head\n",
    "        for _ in range(num_heads)\n",
    "    ]\n",
    "\n",
    "# Generate separate weight matrices for each head\n",
    "multi_head_weights = generate_weights(num_heads, head_dim)\n",
    "\n",
    "# Define softmax function for numerical stability\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Perform multi-head attention\n",
    "all_attention_outputs = []\n",
    "for head_index, (W_q, W_k, W_v) in enumerate(multi_head_weights):\n",
    "    # Slice the sentence embeddings for this head\n",
    "    # Each head will process a different projection of the embeddings\n",
    "    head_embeddings = sentence_embeddings[:, head_index * head_dim : (head_index + 1) * head_dim]\n",
    "\n",
    "    # Compute Queries (Q), Keys (K), and Values (V) for this head\n",
    "    Q = head_embeddings @ W_q  # Transform embeddings into Queries\n",
    "    K = head_embeddings @ W_k  # Transform embeddings into Keys\n",
    "    V = head_embeddings @ W_v  # Transform embeddings into Values\n",
    "\n",
    "    # Compute attention scores using scaled dot-product attention\n",
    "    scores = Q @ K.T  # Compute raw attention scores (similarity between Q and K)\n",
    "    d_k = K.shape[1]  # Dimension of keys\n",
    "    scores /= np.sqrt(d_k)  # Scale scores for numerical stability\n",
    "\n",
    "    # Apply softmax to obtain normalized attention weights\n",
    "    attention_weights = softmax(scores)\n",
    "\n",
    "    # Compute weighted sum of values to get the attention output for this head\n",
    "    attention_output = attention_weights @ V\n",
    "\n",
    "    # Store the attention output for this head\n",
    "    all_attention_outputs.append(attention_output)\n",
    "\n",
    "    # Print detailed information for this head\n",
    "    print(f\"Head {head_index + 1}:\")\n",
    "    print(\"Attention Weights:\")\n",
    "    print(attention_weights)\n",
    "    print(\"Attention Output:\")\n",
    "    print(attention_output)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Concatenate the outputs from all heads to form the final multi-head attention output\n",
    "final_attention_output = np.concatenate(all_attention_outputs, axis=1)\n",
    "\n",
    "# Print the final multi-head attention output\n",
    "print(\"Final Multi-Head Attention Output:\")\n",
    "print(final_attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eOFhX1zPMaL"
   },
   "source": [
    "## Encoder-Only Models\n",
    "Let's introduce the Bidirectional encoder representations from transformers (BERT) to create sentence embeddings.  The BERT model represents words with a 768-dimensional vector that captures the contextual meaning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "e92bb75990f14336989d1267286b47a3"
     ]
    },
    "id": "VXEYv9DcPMaM",
    "outputId": "cee4bc4d-23cd-49d8-83a9-89c768015df6"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch\n",
    "\n",
    "# Import required libraries\n",
    "from transformers import BertTokenizer, BertModel  # Pretrained BERT tokenizer and model\n",
    "import torch  # For tensor manipulation (similar to NumPy)\n",
    "\n",
    "# Load a pretrained BERT model and tokenizer\n",
    "# BERT base uncased: lowercase version of the model trained on English text\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in evaluation mode (not training)\n",
    "model.eval()\n",
    "\n",
    "# Step 1: Define a sentence for embedding\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# Step 2: Tokenize the sentence\n",
    "# Tokenization converts the sentence into tokens that BERT understands.\n",
    "# BERT uses WordPiece tokenization, which splits words into subwords if necessary.\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")  # \"pt\" indicates PyTorch tensors\n",
    "print(\"Tokenized Input IDs:\", inputs['input_ids'])\n",
    "print(\"Attention Mask:\", inputs['attention_mask'])\n",
    "\n",
    "# Explanation of Tokenized Output:\n",
    "# input_ids: Each word/subword is converted to an integer representing its vocabulary index.\n",
    "# attention_mask: A binary mask indicating which tokens are real (1) and which are padding (0).\n",
    "\n",
    "# Step 3: Pass the tokenized inputs through the BERT model\n",
    "with torch.no_grad():  # Disable gradient calculation (not needed for inference)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Step 4: Extract the hidden states from BERT's output\n",
    "# BERT returns two outputs: the last hidden state and the pooled output.\n",
    "# The last hidden state contains embeddings for each token in the input sentence.\n",
    "last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# Step 5: Average the token embeddings to create a sentence embedding\n",
    "# This is a simple way to get a fixed-size vector representing the entire sentence.\n",
    "sentence_embedding = last_hidden_state.mean(dim=1)  # Average across the sequence length dimension\n",
    "\n",
    "# Print the sentence embedding\n",
    "print(\"Sentence Embedding Shape:\", sentence_embedding.shape)\n",
    "print(\"Sentence Embedding Vector:\", sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdNYbHZ0PMaM"
   },
   "source": [
    "## Text Classification with BERT\n",
    "Let’s use the Hugging Face Transformers library to fine-tune BERT for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AP-5cLltPMaM",
    "outputId": "4ba9a058-f8e1-47b6-bec5-d753bbf68f80"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model for classification\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Example input text\n",
    "texts = [\"I love this product!\", \"The movie was terrible.\"]\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Convert logits to probabilities and predictions\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "predictions = torch.argmax(probs, dim=1)\n",
    "\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predictions:\", predictions)  # 0 for negative, 1 for positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dy1OexKPMaM"
   },
   "source": [
    "## Text Generation with GPT\n",
    "Let’s generate text using a pre-trained GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MS5p-lsSPMaM",
    "outputId": "5d91099a-c11b-448e-ed3f-b49074073746"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained GPT tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Once upon a time in a faraway land,\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(inputs, max_length=50, do_sample=True, temperature=0.1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZknMovKOPMaN"
   },
   "source": [
    "## Language Translation with T5\n",
    "Let’s translate a sentence using the T5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53rW5o2KPMaN",
    "outputId": "901b28b8-2dea-47b2-bbce-6162b352a268"
   },
   "outputs": [],
   "source": [
    "# uinstall necessary libraries\n",
    "!pip install sentencepiece\n",
    "\n",
    "#load libraries\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Input text for translation\n",
    "input_text = \"translate English to French: The weather is sunny.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate translation\n",
    "output = model.generate(inputs.input_ids, max_length=50)\n",
    "\n",
    "# Decode the generated translation\n",
    "translation = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Translation:\", translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ANXTFVEPMaN"
   },
   "source": [
    "Let's use T5 for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSWZ08iiPMaN",
    "outputId": "4518a401-b41b-4284-bea0-a72a08098262"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")  # \"t5-small\" is a smaller, faster version of T5\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Input text for summarization\n",
    "text = \"\"\"\n",
    "The Mona Lisa is a half-length portrait painting by the Italian artist Leonardo da Vinci.\n",
    "It is considered an archetypal masterpiece of the Italian Renaissance, and it has been described\n",
    "as the most famous, most visited, most written about, and most sung about work of art in the world.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the text for the T5 model\n",
    "# T5 treats all tasks as text-to-text; for summarization, we prepend \"summarize: \" to the input text\n",
    "input_text = \"summarize: \" + text\n",
    "\n",
    "# Tokenize the input text and convert it to a PyTorch tensor\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(\n",
    "    inputs,\n",
    "    max_length=50,          # Maximum length of the summary\n",
    "    num_beams=4,            # Beam search with 4 beams for more coherent output\n",
    "    early_stopping=True     # Stop once an optimal summary is found\n",
    ")\n",
    "\n",
    "# Decode the generated summary back to text\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary\n",
    "print(\"Summary:\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
