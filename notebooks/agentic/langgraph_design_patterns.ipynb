{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMwnZ6QgWevJ"
      },
      "source": [
        "# LangGraph Design Patterns\n",
        "\n",
        "#TODO: give me a brief two-sentence description of the notebook and then an outline of all of sections with a bried desctription of what we do in each of the sections\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "752kkDKkWevK"
      },
      "source": [
        "## Set up the Environment and Components\n",
        "\n",
        "#TODO provide two sentence descripton of the section"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core langgraph ipython-extensions typing-extensions langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USDhPjmdXaKm",
        "outputId": "5a57faf5-ec6f-4bbb-979c-b769f02b43b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.45)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: ipython-extensions in /usr/local/lib/python3.11/dist-packages (0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.12.2)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (0.3.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.10.6)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.21)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.58)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.66.3 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.66.3)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.27.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.66.3->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (2.3.0)\n",
            "Downloading langchain_openai-0.3.9-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain_openai\n",
            "Successfully installed langchain_openai-0.3.9 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tw7eXkcWevL"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "import random\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1QhXQOzWevM"
      },
      "source": [
        "Setting up a connection to a proprietary remote model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCeVap9hWevM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b647912e-6280-44a2-b128-ccbf10826739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n"
          ]
        }
      ],
      "source": [
        "# # Set OpenAI API Key\n",
        "# import os, getpass\n",
        "\n",
        "# def _set_env(var: str):\n",
        "#     if not os.environ.get(var):\n",
        "#         os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "# _set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading the `config.json` file\n",
        "import json\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "# Load the JSON file and extract values\n",
        "file_name = 'config.json'\n",
        "with open(file_name, 'r') as file:\n",
        "    config = json.load(file)\n",
        "    os.environ['OPENAI_API_KEY'] = config.get(\"API_KEY\") # Loading the API Key\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = config.get(\"OPENAI_API_BASE\") # Loading the API Base Url"
      ],
      "metadata": {
        "id": "vyyi5eOnYqRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlBBezSKWevM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805069e8-081c-4640-be64-29320b947531"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6ec83003ad', 'id': 'chatcmpl-BEcMQYNSwNKVOq9IMeCwzcbW1N6id', 'finish_reason': 'stop', 'logprobs': None}, id='run-86ac577b-d70e-4406-b35e-045096138ff4-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "gpt4o_chat.invoke(\"hello world\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usdmz8RSWevN"
      },
      "source": [
        "Create some tools for the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqP1jl4kWevN"
      },
      "outputs": [],
      "source": [
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSsOC6hOWevN"
      },
      "outputs": [],
      "source": [
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP1_OinKWevN"
      },
      "outputs": [],
      "source": [
        "def determine_outcome(bet: int) -> str:\n",
        "    \"\"\"Roll a pair of dice and determine wether the sum of that roll is less than the bet\n",
        "\n",
        "    Args:\n",
        "        bet: the number that you believe the dice sum will be less than or equal to (number between 1 and 12)\n",
        "    \"\"\"\n",
        "    roll = random.randint(1, 6) + random.randint(1, 6)\n",
        "\n",
        "    return 'success' if roll <= bet else 'failure'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f8TNoFyWevO"
      },
      "source": [
        "Bind the tools to the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEKfQhPwWevO"
      },
      "outputs": [],
      "source": [
        "llm_with_tools = gpt4o_chat.bind_tools([multiply, determine_outcome])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKmIdc90WevO"
      },
      "outputs": [],
      "source": [
        "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\",)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3h2oOGlWevO"
      },
      "outputs": [],
      "source": [
        "tool_call.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-EXyQe6WevO"
      },
      "source": [
        "## Set up the State\n",
        "\n",
        "#TODO provide two sentence descripton of the section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgWRqFY3WevO"
      },
      "outputs": [],
      "source": [
        "class MessagesState(MessagesState):\n",
        "    # Add any keys needed beyond messages, which is pre-built\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKN5u_R_WevO"
      },
      "source": [
        "A brief look at reducers\n",
        "Reducers allow us to specify how state updates are performed. If no reducer function is specified, then it is assumed that updates to the key should override it as we saw before.\n",
        "\n",
        "To append messages, we can use the pre-built add_messages reducer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnl1HghJWevO"
      },
      "outputs": [],
      "source": [
        "# Initial state\n",
        "initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),\n",
        "                    HumanMessage(content=\"I need help understanding probability theory.\", name=\"User\")\n",
        "                   ]\n",
        "\n",
        "# New message to add\n",
        "new_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")\n",
        "\n",
        "# Test\n",
        "add_messages(initial_messages , new_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTk169koWevP"
      },
      "source": [
        "## Create a Chain\n",
        "\n",
        "#TODO provide two sentence descripton of the section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwcLdS9eWevP"
      },
      "outputs": [],
      "source": [
        "def tool_calling_llm(state: MessagesState):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Build graph\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_edge(\"tool_calling_llm\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRbqDFO-WevP"
      },
      "outputs": [],
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLrg6iNKWevP"
      },
      "outputs": [],
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 5 and 7\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIA2vIiTWevP"
      },
      "outputs": [],
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Bet 6 against a dice roll\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YscrukoPWevP"
      },
      "source": [
        "## Create a Router\n",
        "\n",
        "#TODO provide two sentence descripton of the section. Make sure to talk about the LangGraph built in ToolNode and tools_condition for the conditional edge. Also decribe how this one is different than our previous chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95F_fQtrWevP"
      },
      "outputs": [],
      "source": [
        "# Node\n",
        "def tool_calling_llm(state: MessagesState):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Build graph\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "builder.add_node(\"tools\", ToolNode([multiply, add, determine_outcome]))\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_conditional_edges(\n",
        "    \"tool_calling_llm\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaD8_2DtWevP"
      },
      "outputs": [],
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vUAGCpCWevP"
      },
      "outputs": [],
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 5 and 7\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HTWQrCFWevP"
      },
      "outputs": [],
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Bet 6 against a dice roll\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmCJKX4cWevP"
      },
      "source": [
        "## Create an Agent\n",
        "\n",
        "#TODO provide two sentence descripton of the section. Make sure to talk about the agent node we are creating and how this design pattern is different than the router one. Also walkthrough the execution of the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s568HqiXWevP"
      },
      "outputs": [],
      "source": [
        "# System message\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic to determine a bet and then resolving wether that bet passes a dice roll test.\")\n",
        "\n",
        "# Node\n",
        "def assistant(state: MessagesState):\n",
        "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVMINsXqWevQ"
      },
      "outputs": [],
      "source": [
        "# Graph\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode([add, multiply, determine_outcome]))\n",
        "\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "react_graph = builder.compile()\n",
        "\n",
        "# Show\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTFmcpAFWevQ"
      },
      "outputs": [],
      "source": [
        "messages = [HumanMessage(content=\"My defender has a +2 defense. The attack has a +1 modifier for range and +2 modifier for moving. \"\n",
        "\"I need to determine wether my defender passes the attack role by summing together the defense and modifers and then checking the outcome versus a dice roll.\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA2DA2VcWevQ"
      },
      "outputs": [],
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV3o7l3BWevQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}